{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preparation Phase"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "def get_summarizer():\n",
    "    model_name = 'google/pegasus-xsum'\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    summarizer_model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    return tokenizer, summarizer_model\n",
    "\n",
    "# write function that calculates and returns tf-idf scores of all words in a text using libraries\n",
    "def get_tfidf_scores(documents):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit(documents)\n",
    "    return tfidf_vectorizer\n",
    "\n",
    "# select the top n percent of words for each document with highest tf-idf scores\n",
    "def get_top_n_words(document, tfidf_vectorizer, n):\n",
    "    tfidf_scores = tfidf_vectorizer.transform([document])\n",
    "    sorted_indices = tfidf_scores.argsort().flatten()[::-1]\n",
    "    top_n_indices = sorted_indices[:int(len(sorted_indices) * n)]\n",
    "    return [tfidf_vectorizer.get_feature_names()[i] for i in top_n_indices]\n",
    "\n",
    "def read_xml_files():\n",
    "    documents = []\n",
    "    for filename in os.listdir('data'):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            tree = ET.parse('data/' + filename)\n",
    "            root = tree.getroot()\n",
    "            for sec in root:\n",
    "                if len(documents) > 10:\n",
    "                    break\n",
    "                text = sec.find(\".//AbstractText\")\n",
    "                if text != None:\n",
    "                    if text.text != None:\n",
    "                        if len(text.text) > 1500:\n",
    "                            documents.append(text.text)\n",
    "            print(\"finished doc\" + filename)\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "def read_corpus(documents, tokens_only=False):\n",
    "    i = 0\n",
    "    for doc in documents:\n",
    "        tokens = simple_preprocess(doc)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield TaggedDocument(tokens, [i])\n",
    "        i = i+1\n",
    "\n",
    "def most_similar(text):\n",
    "    processed_query = simple_preprocess(text)\n",
    "    v1 = model.infer_vector(processed_query)\n",
    "    return model.docvecs.most_similar([v1])\n",
    "\n",
    "def summarize(text):\n",
    "    batch = tokenizer([text], truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "    translated = summarizer_model.generate(**batch)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "def train_embedding(train_corpus, vecotor_size=124, window=20, min_count=2, epochs=200):\n",
    "    model = Doc2Vec(vector_size=vecotor_size, window=window, min_count=min_count, epochs=epochs, workers=10)\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# create class Doc2VecModel that implements BaseEstimator\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Doc2VecModel(BaseEstimator):\n",
    "    def __init__(self, vector_size=124, window=20, min_count=2, epochs=200, dm=0, hs=0, dbow_words=0, sample=1e-3):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.dm = dm\n",
    "        self.hs = hs\n",
    "        self.dbow_words = dbow_words\n",
    "        self.sample = sample\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "        self.X_test = query_summarized\n",
    "        self.X = train_corpus\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model = train_embedding(train_corpus, self.vector_size, self.window, self.min_count, self.epochs)\n",
    "        return self\n",
    "\n",
    "    def most_similar(self, text):\n",
    "        processed_query = simple_preprocess(text)\n",
    "        v1 = self.model.infer_vector(processed_query)\n",
    "        return self.model.docvecs.most_similar([v1])\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if not X_test:\n",
    "            X_test = self.X_test\n",
    "        return self.most_similar(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tokenizer, summarizer_model = get_summarizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished docpubmed22n1109.xml\n"
     ]
    }
   ],
   "source": [
    "documents = read_xml_files()\n",
    "train_corpus = list(read_corpus(documents))\n",
    "test_corpus = list(read_corpus(documents, tokens_only=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(train_corpus))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = train_embedding(train_corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\anaconda3\\envs\\HackZurich\\lib\\site-packages\\transformers\\generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 64 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_full:\n",
      " Medical education has increasingly shifted towards replacing large lectures with a combination of online and smaller in-person group sessions. This study compares the efficacy of a virtual Opioid Overdose Prevention and Response Training (OOPRT) for first-year medical students with an identical in-person training. During their first unit of medical school, students in the class of 2023 (cohort 1) received OOPRT in-person and students in the class of 2024 (cohort 2) received training via Zoom. Aside from the delivery format, trainings were identical. Both cohorts completed identical surveys at medical school entry and post-training to evaluate knowledge and experiences using the Opioid Overdose Knowledge Scale, Opioid Overdose Attitudes Scale, Medical Conditions Regard Scale, and Naloxone Related Risk Compensation Beliefs. Of 430 students, 84.2% (362: 124 in cohort 1; 238 in cohort 2) completed baseline and post-training surveys. Students reported significantly improved opioid overdose knowledge and attitudes in all 4 knowledge and 3 attitudes subscales after training. Only one outcome differed by training type: knowledge of opioid overdose signs. Cohorts did not differ in opinions of training; 97.2% enjoyed it and 99.4% believed future classes should receive it. Medical students' attitudes and knowledge significantly improved after OOPRT; only one of 13 outcomes showed a cohort difference. There were no differences in enjoyment, indicating that switching to virtual learning does not undermine the learning experience. Further studies are needed to confirm that these results can be extended to other medical school topics where small group interactive discussion is preferred.\n",
      "\n",
      "query_summarized:\n",
      " Virtual opioid overdose prevention training does not undermine the learning experience for first-year medical students.\n"
     ]
    }
   ],
   "source": [
    "test_query_full = documents[0]\n",
    "\n",
    "# print the summary\n",
    "query_summarized = summarize(test_query_full)\n",
    "print(\"query_full:\\n\", test_query_full)\n",
    "print()\n",
    "print(\"query_summarized:\\n\", query_summarized)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Virtual opioid overdose prevention training does not undermine the learning experience for first-year medical students.\n",
      "Most similar text:\n",
      " [(0, 0.42832982540130615), (5, 0.24744363129138947), (2, 0.20902539789676666), (3, 0.1938617080450058), (10, 0.14480555057525635), (9, 0.010960517451167107), (4, -0.017481787130236626), (1, -0.033810585737228394), (7, -0.1667816936969757), (8, -0.234575554728508)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_3452\\3603518407.py:58: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  return model.docvecs.most_similar([v1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input text:\", query_summarized)\n",
    "print(\"Most similar text:\\n\", most_similar(query_summarized))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Case"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"breast cancer treatment\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Input text:\", query)\n",
    "most_similar_result = most_similar(query)\n",
    "print(\"Most similar text:\\n\", most_similar_result)\n",
    "print(\"Summary of most similar text:\\n\", summarize(documents[most_similar_result[0][0]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec enrichment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# concatenate all documents to a single string\n",
    "all_documents = \"\"\n",
    "for doc in documents:\n",
    "    all_documents += doc\n",
    "\n",
    "# train word2vec model with all documents\n",
    "word2vec_model = Word2Vec([all_documents.split()], window=5, min_count=1, workers=10, epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenize the input sentence and return for each word the closest word from the word2vec model\n",
    "def get_enriched_words(query):\n",
    "    enriched_words = []\n",
    "    for word in query.split():\n",
    "        enriched_words.append(word2vec_model.wv.most_similar(word))\n",
    "    return enriched_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_text = \"breast cancer treatment\"\n",
    "print(\"Input text:\", input_text)\n",
    "enriched_words = get_enriched_words(input_text)\n",
    "for i, word in enumerate(input_text.split()):\n",
    "    print(\"Word:\", word)\n",
    "    print(\"Enriched words:\")\n",
    "    for enriched_word in enriched_words[i]:\n",
    "        print(enriched_word)\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters from random search:\n",
      "{'window': 5, 'vector_size': 68, 'sample': 0.0, 'min_count': 1, 'hs': 0, 'epochs': 200, 'dm': 2, 'dbow_words': 1}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning with random search for train_embedding function\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# define the parameter values that should be searched\n",
    "# vector_size from 50 to 200 in steps of 10\n",
    "# window from 5 to 30 in steps of 5\n",
    "# min_count from 1 to 5 in steps of 1\n",
    "# epochs from 100 to 1000 in steps of 100\n",
    "param_dist = {\"vector_size\": list(range(44, 116, 12)),\n",
    "              \"window\": list(range(5, 20, 5)),\n",
    "              \"min_count\": list(range(1, 5, 1)),\n",
    "              \"epochs\": list(range(100, 300, 100)),\n",
    "              \"dm\": list(range(0, 3, 1)),\n",
    "              \"hs\": list(range(0, 2, 1)),\n",
    "              \"dbow_words\": list(range(0, 2, 1)),\n",
    "              \"sample\": list(np.arange(0, 0.001, 0.0005))}\n",
    "\n",
    "# create metric function\n",
    "def custom_metric(y_true, ys_pred):\n",
    "    # check if any tuple in ys_pred contains a 0 at index 0 and get the index of the tuple\n",
    "    index = next((i for i, x in enumerate(ys_pred) if x[0] == 0), None)\n",
    "    # if no tuple contains a 0 at index 0, return 0\n",
    "    if index == None:\n",
    "        return 0\n",
    "    # else return the value at index 1 of the tuple at index\n",
    "    else:\n",
    "        return ys_pred[index][1]\n",
    "\n",
    "# instantiate the random search with your own metric function\n",
    "model = Doc2VecModel()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, scoring=custom_metric, n_iter=10, cv=3, verbose=10, n_jobs=-1, random_state=42)\n",
    "\n",
    "# fit the random search model\n",
    "random_search.fit(train_corpus)\n",
    "\n",
    "# view the best parameters from the random search\n",
    "print(\"Best parameters from random search:\")\n",
    "print(random_search.best_params_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}